{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import train_test_split\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, name):\n        PAD_token = 0   # Used for padding short sentences\n        unk_token = 1\n        SOS_token = 2   # Start-of-sentence token\n        EOS_token = 3   # End-of-sentence token\n        \n        self.name = name\n        self.word2index = {\"[PAD]\" : PAD_token,\"[UNK]\" : unk_token, \"[SOS]\" : SOS_token, \"[EOS]\" : EOS_token}\n        self.word2count = {}\n        self.index2word = {PAD_token: \"[PAD]\", unk_token: \"[UNK]\", SOS_token: \"[SOS]\", EOS_token: \"[EOS]\" }\n        self.num_words = 4\n        self.num_sentences = 0\n        self.longest_sentence = 0\n\n    def add_word(self, word):\n        if word not in self.word2index:\n            # First entry of word into vocabulary\n            self.word2index[word] = self.num_words\n            self.word2count[word] = 1\n            self.index2word[self.num_words] = word\n            self.num_words += 1\n        else:\n            # Word exists; increase word count\n            self.word2count[word] += 1\n            \n    def add_sentence(self, sentence):\n        sentence = str(sentence)\n        sentence_len = 0\n        for word in sentence.split(' '):\n            sentence_len += 1\n            self.add_word(word)\n        if sentence_len > self.longest_sentence:\n            # This is the longest sentence\n            self.longest_sentence = sentence_len\n        # Count the number of sentences\n        self.num_sentences += 1\n\n    def to_word(self, index):\n        return self.index2word[index]\n\n    def to_index(self, word):\n        return self.word2index[word]\n    \n    def encode(self, st):\n        tokens = []\n        for i in st.split():\n            try:\n                tokens.append(self.to_index(i))\n            except:\n                tokens.append(self.to_index(\"[UNK]\"))\n        return tokens\n    def decode(self, tokens):\n        st = []\n        for i in tokens:\n            st.append(self.to_word(i))\n        return \" \".join(st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/code-mixed-telugu/cleaned data.csv\", dtype=str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp_lang = Vocabulary(\"input\")\ntarg_lang = Vocabulary(\"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(df.shape[0]):\n    inp_lang.add_sentence(df.loc[k,'text'])\nfor k in range(df.shape[0]):\n    targ_lang.add_sentence(df.loc[k,'target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 74\nct = df.shape[0]\ninput_ids = []\nattention_mask = []\ntoken_type_ids = []\ntarget_ids = []\n\nfor k in range(df.shape[0]):\n    tokens = inp_lang.encode(str(df.loc[k,'text']))\n    target_toks = targ_lang.encode(str(df.loc[k,'target']))\n    \n    if len(tokens)>70:\n        tokens = tokens[:70]\n    if len(target_toks)>70:\n        target_toks = target_toks[:70]\n    \n    \n    tokens = [2] + tokens + [3]\n    target_toks = [2] + target_toks + [3]\n    input_ids.append(tokens)\n    target_ids.append(target_toks)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_ids,\n                                                         padding='post')\ntarget_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_ids,\n                                                         padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.15)\n\n# Show length\nprint(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 64\nsteps_per_epoch = len(input_tensor_train)//BATCH_SIZE\nprint(steps_per_epoch)\nembedding_dim = 256\nunits = 1024\n\nvocab_inp_size = len(inp_lang.word2index)+1\nvocab_tar_size = len(targ_lang.word2index)+1\n\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_inp_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_input_batch, example_target_batch = next(iter(dataset))\nexample_input_batch.shape, example_target_batch.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n        super(Encoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.enc_units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state = hidden)\n        return output, state\n\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_sz, self.enc_units))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n\n# sample input\nsample_hidden = encoder.initialize_hidden_state()\nsample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\nprint ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\nprint ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, query, values):\n        # query hidden state shape == (batch_size, hidden size)\n        # query_with_time_axis shape == (batch_size, 1, hidden size)\n        # values shape == (batch_size, max_len, hidden size)\n        # we are doing this to broadcast addition along the time axis to calculate the score\n        query_with_time_axis = tf.expand_dims(query, 1)\n\n        # score shape == (batch_size, max_length, 1)\n        # we get 1 at the last axis because we are applying score to self.V\n        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n        score = self.V(tf.nn.tanh(\n            self.W1(query_with_time_axis) + self.W2(values)))\n\n        # attention_weights shape == (batch_size, max_length, 1)\n        attention_weights = tf.nn.softmax(score, axis=1)\n\n        # context_vector shape after sum == (batch_size, hidden_size)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        return context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attention_layer = BahdanauAttention(10)\nattention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n\nprint(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\nprint(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n        super(Decoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.dec_units = dec_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n        self.fc = tf.keras.layers.Dense(vocab_size)\n\n        # used for attention\n        self.attention = BahdanauAttention(self.dec_units)\n\n    def call(self, x, hidden, enc_output):\n        # enc_output shape == (batch_size, max_length, hidden_size)\n        context_vector, attention_weights = self.attention(hidden, enc_output)\n\n        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n        x = self.embedding(x)\n\n        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n        # passing the concatenated vector to the GRU\n        output, state = self.gru(x)\n\n        # output shape == (batch_size * 1, hidden_size)\n        output = tf.reshape(output, (-1, output.shape[2]))\n\n        # output shape == (batch_size, vocab)\n        x = self.fc(output)\n\n        return x, state, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n\nsample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n                                      sample_hidden, sample_output)\n\nprint ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = '/kaggle/working/'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_step(inp, targ, enc_hidden):\n    loss = 0\n\n    with tf.GradientTape() as tape:\n        enc_output, enc_hidden = encoder(inp, enc_hidden)\n\n        dec_hidden = enc_hidden\n\n        dec_input = tf.expand_dims([targ_lang.word2index[\"[SOS]\"]] * BATCH_SIZE, 1)\n\n        # Teacher forcing - feeding the target as the next input\n        for t in range(1, targ.shape[1]):\n            # passing enc_output to the decoder\n            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n\n            loss += loss_function(targ[:, t], predictions)\n\n            # using teacher forcing\n            dec_input = tf.expand_dims(targ[:, t], 1)\n\n    batch_loss = (loss / int(targ.shape[1]))\n\n    variables = encoder.trainable_variables + decoder.trainable_variables\n\n    gradients = tape.gradient(loss, variables)\n\n    optimizer.apply_gradients(zip(gradients, variables))\n\n    return batch_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 25\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    enc_hidden = encoder.initialize_hidden_state()\n    total_loss = 0\n\n    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss = train_step(inp, targ, enc_hidden)\n        total_loss += batch_loss\n\n        if batch % 40 == 0:\n            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                       batch,\n                                                       batch_loss.numpy()))\n    # saving (checkpoint) the model every 2 epochs\n    if (epoch + 1) % 5 == 0:\n        checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                      total_loss / steps_per_epoch))\n    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length_targ, max_length_inp = 72, 72","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(sentence):\n    attention_plot = np.zeros((max_length_targ, max_length_inp))\n\n    #sentence = preprocess_sentence(sentence)\n    sentence = \"[SOS] \"+sentence+\" [EOS]\"\n\n    inputs = inp_lang.encode(sentence)\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                         maxlen=max_length_inp,\n                                                         padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n\n    result = ''\n\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([targ_lang.word2index['[SOS]']], 0)\n\n    for t in range(max_length_targ):\n        predictions, dec_hidden, attention_weights = decoder(dec_input,\n                                                             dec_hidden,\n                                                             enc_out)\n\n        # storing the attention weights to plot later on\n        attention_weights = tf.reshape(attention_weights, (-1, ))\n        attention_plot[t] = attention_weights.numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n\n        result += targ_lang.index2word[predicted_id] + ' '\n\n        if targ_lang.index2word[predicted_id] == '[EOS]':\n            return result, sentence, attention_plot\n\n        # the predicted ID is fed back into the model\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result, sentence, attention_plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def translate(sentence):\n    result, sentence, attention_plot = evaluate(sentence)\n\n    print('Input: %s' % (sentence))\n    print('Predicted translation: {}'.format(result))\n\n    #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n    #plot_attention(attention_plot, sentence.split(' '), result.split(' '))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(df.sample(n=10,random_state=42).shape[0]):\n    translate(df.loc[k,'text'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}